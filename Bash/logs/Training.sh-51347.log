Running via sbatch on puck5 on Tue Nov 18 04:18:08 PM CET 2025
Python 3.10.13
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/tleludec/.netrc
wandb: Currently logged in as: tiphaine-leludec (tiphaine-leludec-ens) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 89bnmsq0
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /home/tleludec/Transcription_whisper/Code/fine-tuning-whisper/Script/wandb/run-20251118_161824-89bnmsq0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-paper-67
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tiphaine-leludec-ens/whisper_lora
wandb: üöÄ View run at https://wandb.ai/tiphaine-leludec-ens/whisper_lora/runs/89bnmsq0
The model is already on multiple devices. Skipping the move to device specified in `args`.
You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
trainable params: 110,592 || all params: 241,845,504 || trainable%: 0.0457
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
  0%|          | 0/4 [00:00<?, ?it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.07it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.02it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.36s/it]Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Predictions type: <class 'numpy.ndarray'>
Decoded Predictions: [" Hier matin, je suis all√© me promener au jarabin o√π on a notre petit potager qu'on loue √† la ville de Vendeur et", ' On a vu notre voisin de jardin qui nous a propos√© de nous donner un petit tabus, un petit figuier. Et donc on a pris le figuier et puis on a fait le trou pour le planter.', " En faisant le trou, la b√™che a cass√©. J'ai pris la b√™che, ma le manche qui a cass√©. J'ai pris la b√™che de la b√™che dans l'≈ìil. Voil√†, petite blessure, pas grand chose. √áa a fait tr√®s mal sur le coup quelques minutes pour r√©cup√©rer.", " et puis voil√† ma compagne a continu√© √† planter le figier pendant que moi je r√©cup√©rais normal et puis du coup on avait pr√©vu de se balader plus longtemps mais on est rentr√© parce que voil√† j'avais besoin quand m√™me de prendre quelque chose de mettre de la glace sur sur ma tente", ' Voil√†.']
Decoded Labels: [" Hier matin, je suis all√© me promener au jarabin o√π on a notre petit potager qu'on loue √† la ville de Vendeur et", ' On a vu notre voisin de jardin qui nous a propos√© de nous donner un petit tabus, un petit figuier. Et donc on a pris le figuier et puis on a fait le trou pour le planter.', " En faisant le trou, la b√™che a cass√©. J'ai pris la b√™che, ma le manche qui a cass√©. J'ai pris la b√™che de la b√™che dans l'≈ìil. Voil√†, petite blessure, pas grand chose. √áa a fait tr√®s mal sur le coup quelques minutes pour r√©cup√©rer.", " et puis voil√† ma compagne a continu√© √† planter le figier pendant que moi je r√©cup√©rais normal et puis du coup on avait pr√©vu de se balader plus longtemps mais on est rentr√© parce que voil√† j'avais besoin quand m√™me de prendre quelque chose de mettre de la glace sur sur ma tente", ' Voil√†.']
WER Score: 1.1142061281337048
Traceback (most recent call last):
  File "/home/tleludec/Transcription_whisper/Code/fine-tuning-whisper/Script/dataset_hugging_face.py", line 245, in <module>
    trainer.evaluate()
  File "/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/transformers/trainer.py", line 4519, in evaluate
    self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)
  File "/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/transformers/trainer_callback.py", line 538, in on_evaluate
    return self.call_event("on_evaluate", args, state, control, metrics=metrics)
  File "/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/transformers/trainer_callback.py", line 556, in call_event
    result = getattr(callback, event)(
  File "/home/tleludec/Transcription_whisper/Code/fine-tuning-whisper/Script/dataset_hugging_face.py", line 144, in on_evaluate
    wer_value = metrics.get("eval_wer", metrics.get("wer"), metrics.get("eval_eval_wer"))
TypeError: get expected at most 2 arguments, got 3
Traceback (most recent call last):
  File "/home/tleludec/Transcription_whisper/Code/fine-tuning-whisper/Script/dataset_hugging_face.py", line 245, in <module>
    trainer.evaluate()
  File "/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 191, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/transformers/trainer.py", line 4519, in evaluate
    self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)
  File "/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/transformers/trainer_callback.py", line 538, in on_evaluate
    return self.call_event("on_evaluate", args, state, control, metrics=metrics)
  File "/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/transformers/trainer_callback.py", line 556, in call_event
    result = getattr(callback, event)(
  File "/home/tleludec/Transcription_whisper/Code/fine-tuning-whisper/Script/dataset_hugging_face.py", line 144, in on_evaluate
    wer_value = metrics.get("eval_wer", metrics.get("wer"), metrics.get("eval_eval_wer"))
TypeError: get expected at most 2 arguments, got 3
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mvibrant-paper-67[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251118_161824-89bnmsq0/logs[0m
computation end :Tue Nov 18 04:18:37 PM CET 2025
