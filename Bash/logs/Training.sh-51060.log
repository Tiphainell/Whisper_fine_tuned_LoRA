Running via sbatch on puck5 on Thu Nov 13 10:56:31 AM CET 2025
Python 3.10.13
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/tleludec/.netrc
wandb: Currently logged in as: tiphaine-leludec (tiphaine-leludec-ens) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run lyi9s8fu
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /home/tleludec/Transcription_whisper/Code/fine-tuning-whisper/Script/wandb/run-20251113_105651-lyi9s8fu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-disco-43
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tiphaine-leludec-ens/whisper_lora
wandb: üöÄ View run at https://wandb.ai/tiphaine-leludec-ens/whisper_lora/runs/lyi9s8fu
trainable params: 491,520 || all params: 1,543,796,480 || trainable%: 0.0318
  0%|          | 0/8319 [00:00<?, ?it/s]/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.
  warnings.warn(
You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/8319 [00:10<23:28:08, 10.16s/it]  0%|          | 2/8319 [00:16<18:26:28,  7.98s/it]  0%|          | 3/8319 [00:22<16:25:30,  7.11s/it]                                                     0%|          | 3/8319 [00:22<16:25:30,  7.11s/it]Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
{'loss': 3.4307, 'grad_norm': nan, 'learning_rate': 0.0009997595864887606, 'epoch': 0.0}
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']

  0%|          | 0/895 [00:00<?, ?it/s][A
  0%|          | 2/895 [00:05<43:38,  2.93s/it][A
  0%|          | 3/895 [00:11<1:01:42,  4.15s/it][A
  0%|          | 4/895 [00:17<1:11:06,  4.79s/it][A
  1%|          | 5/895 [00:23<1:16:34,  5.16s/it][A
  1%|          | 6/895 [00:29<1:19:57,  5.40s/it][A
  1%|          | 7/895 [00:35<1:22:56,  5.60s/it][A
  1%|          | 8/895 [00:41<1:26:31,  5.85s/it][A
  1%|          | 9/895 [00:48<1:29:30,  6.06s/it][A
  1%|          | 10/895 [00:54<1:31:47,  6.22s/it][A
  1%|          | 11/895 [01:01<1:33:25,  6.34s/it][A
  1%|‚ñè         | 12/895 [01:07<1:33:19,  6.34s/it][A
  1%|‚ñè         | 13/895 [01:14<1:34:43,  6.44s/it][A
  2%|‚ñè         | 14/895 [01:21<1:35:39,  6.51s/it][A
  2%|‚ñè         | 15/895 [01:27<1:36:14,  6.56s/it][A
  2%|‚ñè         | 16/895 [01:34<1:36:21,  6.58s/it][A
  2%|‚ñè         | 17/895 [01:41<1:36:44,  6.61s/it][A
  2%|‚ñè         | 18/895 [01:47<1:36:27,  6.60s/it][A
  2%|‚ñè         | 19/895 [01:54<1:36:17,  6.60s/it][A
  2%|‚ñè         | 20/895 [02:00<1:36:24,  6.61s/it][A
  2%|‚ñè         | 21/895 [02:07<1:36:17,  6.61s/it][A
  2%|‚ñè         | 22/895 [02:13<1:35:14,  6.55s/it][A
  3%|‚ñé         | 23/895 [02:20<1:34:20,  6.49s/it][A
  3%|‚ñé         | 24/895 [02:26<1:34:59,  6.54s/it][A
  3%|‚ñé         | 25/895 [02:33<1:35:28,  6.58s/it][A
  3%|‚ñé         | 26/895 [02:40<1:35:44,  6.61s/it][A
  3%|‚ñé         | 27/895 [02:47<1:35:51,  6.63s/it][A
  3%|‚ñé         | 28/895 [02:53<1:35:57,  6.64s/it][A
  3%|‚ñé         | 29/895 [03:00<1:36:04,  6.66s/it][A
  3%|‚ñé         | 30/895 [03:07<1:36:01,  6.66s/it][A
  3%|‚ñé         | 31/895 [03:13<1:35:57,  6.66s/it][A
  4%|‚ñé         | 32/895 [03:18<1:29:05,  6.19s/it][A
  4%|‚ñé         | 33/895 [03:22<1:20:06,  5.58s/it][A
  4%|‚ñç         | 34/895 [03:27<1:13:48,  5.14s/it][A
  4%|‚ñç         | 35/895 [03:31<1:09:24,  4.84s/it][A
  4%|‚ñç         | 36/895 [03:35<1:06:07,  4.62s/it][A
  4%|‚ñç         | 37/895 [03:39<1:03:51,  4.47s/it][A
  4%|‚ñç         | 38/895 [03:43<1:02:17,  4.36s/it][A
  4%|‚ñç         | 39/895 [03:47<1:01:15,  4.29s/it][A
  4%|‚ñç         | 40/895 [03:51<1:00:31,  4.25s/it][A
  5%|‚ñç         | 41/895 [03:58<1:11:10,  5.00s/it][A
  5%|‚ñç         | 42/895 [04:06<1:25:12,  5.99s/it][A
  5%|‚ñç         | 43/895 [04:15<1:35:01,  6.69s/it][A
  5%|‚ñç         | 44/895 [04:23<1:41:42,  7.17s/it][A
  5%|‚ñå         | 45/895 [04:31<1:46:17,  7.50s/it][A
  5%|‚ñå         | 46/895 [04:40<1:49:26,  7.73s/it][A
  5%|‚ñå         | 47/895 [04:48<1:51:38,  7.90s/it][A
  5%|‚ñå         | 48/895 [04:56<1:53:15,  8.02s/it][A
  5%|‚ñå         | 49/895 [05:04<1:54:14,  8.10s/it][A
  6%|‚ñå         | 50/895 [05:13<1:55:00,  8.17s/it][A
  6%|‚ñå         | 51/895 [05:21<1:55:27,  8.21s/it][A
  6%|‚ñå         | 52/895 [05:29<1:55:37,  8.23s/it][A
  6%|‚ñå         | 53/895 [05:38<1:55:22,  8.22s/it][A
  6%|‚ñå         | 54/895 [05:46<1:55:00,  8.20s/it][A
  6%|‚ñå         | 55/895 [05:54<1:54:41,  8.19s/it][Aslurmstepd: error: *** JOB 51060 ON puck5 CANCELLED AT 2025-11-13T11:03:27 ***
