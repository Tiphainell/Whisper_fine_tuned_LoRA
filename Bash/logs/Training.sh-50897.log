Running via sbatch on puck5 on Mon Nov 10 04:44:40 PM CET 2025
Python 3.10.13
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
wandb: Appending key for api.wandb.ai to your netrc file: /home/tleludec/.netrc
wandb: Currently logged in as: tiphaine-leludec (tiphaine-leludec-ens) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 46m1xi89
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /home/tleludec/Transcription_whisper/Code/fine-tuning-whisper/Script/wandb/run-20251110_164501-46m1xi89
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sun-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/tiphaine-leludec-ens/whisper_lora
wandb: üöÄ View run at https://wandb.ai/tiphaine-leludec-ens/whisper_lora/runs/46m1xi89
trainable params: 491,520 || all params: 1,543,796,480 || trainable%: 0.0318
  0%|          | 0/8319 [00:00<?, ?it/s]/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.
  warnings.warn(
You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/tleludec/Transcription_whisper/venv_oberon/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/8319 [00:57<132:53:48, 57.52s/it]  0%|          | 2/8319 [01:29<98:09:26, 42.49s/it]   0%|          | 3/8319 [01:51<76:12:46, 32.99s/it]  0%|          | 4/8319 [02:06<59:55:23, 25.94s/it]  0%|          | 5/8319 [02:12<43:34:07, 18.87s/it]  0%|          | 6/8319 [02:19<34:22:16, 14.88s/it]  0%|          | 7/8319 [02:27<29:06:13, 12.61s/it]  0%|          | 8/8319 [02:34<25:07:25, 10.88s/it]  0%|          | 9/8319 [02:41<21:57:15,  9.51s/it]  0%|          | 10/8319 [02:48<20:01:39,  8.68s/it]                                                      0%|          | 10/8319 [02:48<20:01:39,  8.68s/it]Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.
Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']
{'loss': 4.4403, 'grad_norm': 2.5325124263763428, 'learning_rate': 0.000998918139199423, 'epoch': 0.0}
Batch keys: ['input_features', 'labels']
Batch keys: ['input_features', 'labels']

  0%|          | 0/895 [00:00<?, ?it/s][A
  0%|          | 2/895 [00:28<3:32:31, 14.28s/it][A
  0%|          | 3/895 [00:46<3:53:45, 15.72s/it][A
  0%|          | 4/895 [01:17<5:17:13, 21.36s/it][A
  1%|          | 5/895 [01:36<5:06:41, 20.68s/it][A
  1%|          | 6/895 [02:03<5:37:59, 22.81s/it][A
  1%|          | 7/895 [02:27<5:43:36, 23.22s/it][A
  1%|          | 8/895 [02:54<5:58:15, 24.23s/it][A
  1%|          | 9/895 [03:08<5:11:01, 21.06s/it][A
  1%|          | 10/895 [03:20<4:31:53, 18.43s/it][A
  1%|          | 11/895 [03:38<4:27:45, 18.17s/it][A
  1%|‚ñè         | 12/895 [03:51<4:04:23, 16.61s/it][A
  1%|‚ñè         | 13/895 [04:07<4:01:16, 16.41s/it][A
  2%|‚ñè         | 14/895 [04:23<4:00:25, 16.37s/it][A
  2%|‚ñè         | 15/895 [04:45<4:26:29, 18.17s/it][A
  2%|‚ñè         | 16/895 [04:58<3:59:58, 16.38s/it][A
  2%|‚ñè         | 17/895 [05:15<4:04:29, 16.71s/it][A
  2%|‚ñè         | 18/895 [05:31<4:02:49, 16.61s/it][A
  2%|‚ñè         | 19/895 [05:51<4:16:22, 17.56s/it][A
  2%|‚ñè         | 20/895 [06:11<4:25:56, 18.24s/it][A
  2%|‚ñè         | 21/895 [06:29<4:23:58, 18.12s/it][A
  2%|‚ñè         | 22/895 [06:45<4:14:57, 17.52s/it][A
  3%|‚ñé         | 23/895 [07:03<4:14:45, 17.53s/it][A
  3%|‚ñé         | 24/895 [07:25<4:34:11, 18.89s/it][A
  3%|‚ñé         | 25/895 [07:45<4:42:12, 19.46s/it][A
  3%|‚ñé         | 26/895 [09:17<9:55:52, 41.14s/it][A
  3%|‚ñé         | 27/895 [09:36<8:19:04, 34.50s/it][A
  3%|‚ñé         | 28/895 [09:54<7:04:18, 29.36s/it][A
  3%|‚ñé         | 29/895 [10:23<7:03:16, 29.33s/it][A
  3%|‚ñé         | 30/895 [10:38<6:01:23, 25.07s/it][Aslurmstepd: error: *** JOB 50897 ON puck5 CANCELLED AT 2025-11-10T16:59:03 ***
